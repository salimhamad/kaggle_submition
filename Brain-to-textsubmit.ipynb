{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":106809,"databundleVersionId":13056355,"sourceType":"competition"}],"dockerImageVersionId":31236,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import h5py\nimport torch\nimport os\nimport numpy as np\nimport pandas as pd\nfrom pathlib import Path\nfrom tqdm.auto import tqdm\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.nn.utils.rnn import pad_sequence\n\n# --- Ù¡. Ú•ÛÚ©Ø®Ø³ØªÙ†Û•Ú©Ø§Ù† ---\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nBASE_PATH = '/kaggle/input/brain-to-text-25/t15_copyTask_neuralData/hdf5_data_final'\nCHARS = ['BLANK'] + list(\" abcdefghijklmnopqrstuvwxyz' \")\nCHAR_MAP = {c: i for i, c in enumerate(CHARS)}\n\nclass NeuralSignalDataset(Dataset):\n    def __init__(self, file_paths):\n        self.samples = []\n        for path in tqdm(file_paths, desc=\"Loading Data\"):\n            if not os.path.exists(path): continue\n            with h5py.File(path, 'r') as hf:\n                for key in hf.keys():\n                    try:\n                        f = hf[key]['input_features'][()]\n                        f = (f - np.mean(f, axis=0)) / (np.std(f, axis=0) + 1e-6)\n                        text = \"\"\n                        for l_key in ['transcription', 'sentence']:\n                            if l_key in hf[key]:\n                                val = hf[key][l_key][()]\n                                text = val.decode('utf-8').lower() if isinstance(val, bytes) else str(val).lower()\n                                break\n                        if text:\n                            target = [CHAR_MAP[c] for c in text if c in CHAR_MAP]\n                            if len(target) > 0: self.samples.append((f, target))\n                    except: continue\n    def __len__(self): return len(self.samples)\n    def __getitem__(self, idx):\n        return torch.tensor(self.samples[idx][0]).float(), torch.tensor(self.samples[idx][1]).long()\n\ndef collate_fn(batch):\n    inputs, targets = zip(*batch)\n    input_lens = torch.tensor([len(x) for x in inputs], dtype=torch.long)\n    target_lens = torch.tensor([len(y) for y in targets], dtype=torch.long)\n    inputs_padded = pad_sequence(inputs, batch_first=True)\n    targets_padded = pad_sequence(targets, batch_first=True, padding_value=0)\n    return inputs_padded, targets_padded, input_lens, target_lens\n\n# --- Ù¢. Ù…Û†Ø¯ÛÙ„ ---\nclass BrainTransformer(nn.Module):\n    def __init__(self, input_dim=512, hidden_dim=256, nhead=8, num_layers=4):\n        super().__init__()\n        self.proj = nn.Linear(input_dim, hidden_dim)\n        self.upsample = nn.ConvTranspose1d(hidden_dim, hidden_dim, kernel_size=3, stride=2, padding=1, output_padding=1)\n        self.pos_enc = nn.Parameter(torch.randn(1, 5000, hidden_dim))\n        enc_layer = nn.TransformerEncoderLayer(d_model=hidden_dim, nhead=nhead, dropout=0.1, batch_first=True)\n        self.transformer = nn.TransformerEncoder(enc_layer, num_layers=num_layers)\n        self.fc = nn.Linear(hidden_dim, len(CHARS))\n    def forward(self, x):\n        x = self.proj(x)\n        x = x.transpose(1, 2)\n        x = self.upsample(x)\n        x = x.transpose(1, 2)\n        x = x + self.pos_enc[:, :x.size(1), :]\n        x = self.transformer(x)\n        return self.fc(x)\n\n# --- Ù£. Ú•Ø§Ù‡ÛÙ†Ø§Ù† Ùˆ Ù¾ÛØ´Ø¨ÛŒÙ†ÛŒ ---\ndef start_process():\n    train_files = [str(p) for p in Path(BASE_PATH).rglob('data_train.hdf5')]\n    dataset = NeuralSignalDataset(train_files)\n    loader = DataLoader(dataset, batch_size=16, shuffle=True, collate_fn=collate_fn)\n    \n    model = BrainTransformer().to(DEVICE)\n    optimizer = torch.optim.AdamW(model.parameters(), lr=2e-4)\n    criterion = nn.CTCLoss(blank=0, zero_infinity=True)\n    \n    # Ù„ÛØ±Û•Ø¯Ø§ ØªÛ•Ù†Ù‡Ø§ Ù¢ Ø¦Ø§ÛŒÙ¾Û†Ú© Ø¯Ø§Ù…Ù†Ø§ÙˆÛ• Ø¨Û† Ø¦Û•ÙˆÛ•ÛŒ Ø²ÙˆÙˆ ØªÛ•ÙˆØ§Ùˆ Ø¨ÛØª\n    for epoch in range(2):\n        model.train()\n        pbar = tqdm(loader, desc=f\"Epoch {epoch+1}/2\")\n        for batch in pbar:\n            inputs, targets, in_lens, tar_lens = [b.to(DEVICE) for b in batch]\n            optimizer.zero_grad()\n            logits = model(inputs)\n            log_probs = logits.permute(1, 0, 2).log_softmax(2)\n            loss = criterion(log_probs, targets, in_lens * 2, tar_lens)\n            loss.backward()\n            optimizer.step()\n            pbar.set_postfix(loss=loss.item())\n\n    # ÛŒÛ•Ú©Ø³Û•Ø± Ø¯ÙˆØ§ÛŒ Ú•Ø§Ù‡ÛÙ†Ø§Ù† ÙØ§ÛŒÙ„ÛŒ Submission Ø¯Ø±ÙˆØ³Øª Ø¯Û•Ú©Ø§Øª\n    model.eval()\n    results = []\n    test_files = sorted(list(Path(BASE_PATH).rglob(\"data_test.hdf5\")))\n    with torch.no_grad():\n        for f_path in tqdm(test_files, desc=\"Inference\"):\n            with h5py.File(f_path, \"r\") as hf:\n                keys = sorted(hf.keys(), key=lambda k: int(k.split('_')[1]) if '_' in k else 0)\n                for key in keys:\n                    x = torch.from_numpy(hf[key][\"input_features\"][()]).float().unsqueeze(0).to(DEVICE)\n                    x = (x - x.mean()) / (x.std() + 1e-6)\n                    logits = model(x)\n                    indices = torch.argmax(logits[0], dim=-1).unique_consecutive()\n                    decoded = \"\".join([CHARS[i] for i in indices if i != 0]).strip()\n                    results.append(decoded if decoded else \"silence\")\n    \n    pd.DataFrame({\"id\": range(len(results)), \"text\": results}).to_csv(\"submission.csv\", index=False)\n    print(\"ğŸ ØªÛ•ÙˆØ§Ùˆ! ÙØ§ÛŒÙ„ÛŒ submission.csv Ø¯Ø±ÙˆØ³Øª Ø¨ÙˆÙˆ.\")\n\nif __name__ == \"__main__\":\n    start_process()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-31T21:55:02.433396Z","iopub.execute_input":"2025-12-31T21:55:02.434017Z","iopub.status.idle":"2025-12-31T22:32:34.780072Z","shell.execute_reply.started":"2025-12-31T21:55:02.433989Z","shell.execute_reply":"2025-12-31T22:32:34.779461Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading Data:   0%|          | 0/45 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2246aa5bfd6d46ab87001df5dd8015d1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Epoch 1/2:   0%|          | 0/505 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cbff28fdb937412fae9312e8d0216112"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Epoch 2/2:   0%|          | 0/505 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ec606dbc000d4ea0bebf56a27b2cb9b1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Inference:   0%|          | 0/41 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e4a6a554ae6c409f967bf0ebb39fda4a"}},"metadata":{}},{"name":"stdout","text":"ğŸ ØªÛ•ÙˆØ§Ùˆ! ÙØ§ÛŒÙ„ÛŒ submission.csv Ø¯Ø±ÙˆØ³Øª Ø¨ÙˆÙˆ.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"Approach Summary: Brain-to-Text Transformer with CTC Loss\n\n1. Introduction\nThis approach aims to decode neural signals into text using a deep learning architecture specifically designed for sequence-to-sequence mapping. The model processes high-dimensional neural features and converts them into English characters, enabling communication directly from brain activity.\n\n2. Data Preprocessing\nNormalization: Input features are normalized using Z-score normalization (mean subtraction and division by standard deviation) to ensure stable gradient flow during training.\nTarget Encoding: Text transcriptions are mapped to a character-based vocabulary (a-z, spaces, and punctuation) including a 'BLANK' token for CTC compatibility.\nPadding: Dynamic padding is applied to both neural sequences and target text to handle variable-length inputs within batches.\n3. Model Architecture\nThe architecture, named BrainTransformer, consists of three main components:\nLinear Projection: Reduces the initial 512-dimensional neural input to a 256-dimensional latent space.\nTemporal Upsampling: A ConvTranspose1d layer is utilized to increase the temporal resolution of the signal. This is crucial for CTC loss to ensure the output sequence is long enough to accommodate the predicted text.\nTransformer Encoder: A 4-layer Transformer Encoder with Multi-Head Attention (8 heads) is the core engine. It captures long-range temporal dependencies within the neural signals.\nCTC Output Layer: A final fully connected layer maps the features to character probabilities.\n4. Training Strategy\nLoss Function: Connectionist Temporal Classification (CTC Loss) is used, which allows the model to align the neural signal with the text without requiring time-aligned labels.\nOptimizer: AdamW optimizer with a learning rate of 2eâˆ’4 for better weight decay and generalization.\nHardware: Training is performed on NVIDIA GPUs using PyTorch to ensure high-speed computation.\n5. Inference & Decoding\nDuring inference, the model uses Greedy Decoding. It takes the most probable character at each time step and collapses consecutive duplicates (and removes blanks) to produce the final sentence.","metadata":{}}]}