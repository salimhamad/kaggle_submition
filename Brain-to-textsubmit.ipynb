{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":106809,"databundleVersionId":13056355,"sourceType":"competition"}],"dockerImageVersionId":31236,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import h5py\nimport torch\nimport os\nimport numpy as np\nimport pandas as pd\nfrom pathlib import Path\nfrom tqdm.auto import tqdm\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.nn.utils.rnn import pad_sequence\n\n# --- Ù¡. Ú•ÛÚ©Ø®Ø³ØªÙ†Û•Ú©Ø§Ù† ---\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nBASE_PATH = '/kaggle/input/brain-to-text-25/t15_copyTask_neuralData/hdf5_data_final'\nCHARS = ['BLANK'] + list(\" abcdefghijklmnopqrstuvwxyz' \")\nCHAR_MAP = {c: i for i, c in enumerate(CHARS)}\n\nclass NeuralSignalDataset(Dataset):\nÂ  Â  def __init__(self, file_paths):\nÂ  Â  Â  Â  self.samples = []\nÂ  Â  Â  Â  for path in tqdm(file_paths, desc=\"Loading Data\"):\nÂ  Â  Â  Â  Â  Â  if not os.path.exists(path): continue\nÂ  Â  Â  Â  Â  Â  with h5py.File(path, 'r') as hf:\nÂ  Â  Â  Â  Â  Â  Â  Â  for key in hf.keys():\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  try:\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  f = hf[key]['input_features'][()]\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  f = (f - np.mean(f, axis=0)) / (np.std(f, axis=0) + 1e-6)\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  text = \"\"\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  for l_key in ['transcription', 'sentence']:\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if l_key in hf[key]:\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  val = hf[key][l_key][()]\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  text = val.decode('utf-8').lower() if isinstance(val, bytes) else str(val).lower()\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  break\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if text:\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  target = [CHAR_MAP[c] for c in text if c in CHAR_MAP]\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if len(target) > 0: self.samples.append((f, target))\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  except: continue\nÂ  Â  def __len__(self): return len(self.samples)\nÂ  Â  def __getitem__(self, idx):\nÂ  Â  Â  Â  return torch.tensor(self.samples[idx][0]).float(), torch.tensor(self.samples[idx][1]).long()\n\ndef collate_fn(batch):\nÂ  Â  inputs, targets = zip(*batch)\nÂ  Â  input_lens = torch.tensor([len(x) for x in inputs], dtype=torch.long)\nÂ  Â  target_lens = torch.tensor([len(y) for y in targets], dtype=torch.long)\nÂ  Â  inputs_padded = pad_sequence(inputs, batch_first=True)\nÂ  Â  targets_padded = pad_sequence(targets, batch_first=True, padding_value=0)\nÂ  Â  return inputs_padded, targets_padded, input_lens, target_lens\n\n# --- Ù¢. Ù…Û†Ø¯ÛÙ„ ---\nclass BrainTransformer(nn.Module):\nÂ  Â  def __init__(self, input_dim=512, hidden_dim=256, nhead=8, num_layers=4):\nÂ  Â  Â  Â  super().__init__()\nÂ  Â  Â  Â  self.proj = nn.Linear(input_dim, hidden_dim)\nÂ  Â  Â  Â  self.upsample = nn.ConvTranspose1d(hidden_dim, hidden_dim, kernel_size=3, stride=2, padding=1, output_padding=1)\nÂ  Â  Â  Â  self.pos_enc = nn.Parameter(torch.randn(1, 5000, hidden_dim))\nÂ  Â  Â  Â  enc_layer = nn.TransformerEncoderLayer(d_model=hidden_dim, nhead=nhead, dropout=0.1, batch_first=True)\nÂ  Â  Â  Â  self.transformer = nn.TransformerEncoder(enc_layer, num_layers=num_layers)\nÂ  Â  Â  Â  self.fc = nn.Linear(hidden_dim, len(CHARS))\nÂ  Â  def forward(self, x):\nÂ  Â  Â  Â  x = self.proj(x)\nÂ  Â  Â  Â  x = x.transpose(1, 2)\nÂ  Â  Â  Â  x = self.upsample(x)\nÂ  Â  Â  Â  x = x.transpose(1, 2)\nÂ  Â  Â  Â  x = x + self.pos_enc[:, :x.size(1), :]\nÂ  Â  Â  Â  x = self.transformer(x)\nÂ  Â  Â  Â  return self.fc(x)\n\n# --- Ù£. Ú•Ø§Ù‡ÛÙ†Ø§Ù† Ùˆ Ù¾ÛØ´Ø¨ÛŒÙ†ÛŒ ---\ndef start_process():\nÂ  Â  train_files = [str(p) for p in Path(BASE_PATH).rglob('data_train.hdf5')]\nÂ  Â  dataset = NeuralSignalDataset(train_files)\nÂ  Â  loader = DataLoader(dataset, batch_size=16, shuffle=True, collate_fn=collate_fn)\nÂ  Â Â \nÂ  Â  model = BrainTransformer().to(DEVICE)\nÂ  Â  optimizer = torch.optim.AdamW(model.parameters(), lr=2e-4)\nÂ  Â  criterion = nn.CTCLoss(blank=0, zero_infinity=True)\nÂ  Â Â \nÂ  Â  # Ù„ÛØ±Û•Ø¯Ø§ ØªÛ•Ù†Ù‡Ø§ Ù¢ Ø¦Ø§ÛŒÙ¾Û†Ú© Ø¯Ø§Ù…Ù†Ø§ÙˆÛ• Ø¨Û† Ø¦Û•ÙˆÛ•ÛŒ Ø²ÙˆÙˆ ØªÛ•ÙˆØ§Ùˆ Ø¨ÛØª\nÂ  Â  for epoch in range(2):\nÂ  Â  Â  Â  model.train()\nÂ  Â  Â  Â  pbar = tqdm(loader, desc=f\"Epoch {epoch+1}/2\")\nÂ  Â  Â  Â  for batch in pbar:\nÂ  Â  Â  Â  Â  Â  inputs, targets, in_lens, tar_lens = [b.to(DEVICE) for b in batch]\nÂ  Â  Â  Â  Â  Â  optimizer.zero_grad()\nÂ  Â  Â  Â  Â  Â  logits = model(inputs)\nÂ  Â  Â  Â  Â  Â  log_probs = logits.permute(1, 0, 2).log_softmax(2)\nÂ  Â  Â  Â  Â  Â  loss = criterion(log_probs, targets, in_lens * 2, tar_lens)\nÂ  Â  Â  Â  Â  Â  loss.backward()\nÂ  Â  Â  Â  Â  Â  optimizer.step()\nÂ  Â  Â  Â  Â  Â  pbar.set_postfix(loss=loss.item())\n\nÂ  Â  # ÛŒÛ•Ú©Ø³Û•Ø± Ø¯ÙˆØ§ÛŒ Ú•Ø§Ù‡ÛÙ†Ø§Ù† ÙØ§ÛŒÙ„ÛŒ Submission Ø¯Ø±ÙˆØ³Øª Ø¯Û•Ú©Ø§Øª\nÂ  Â  model.eval()\nÂ  Â  results = []\nÂ  Â  test_files = sorted(list(Path(BASE_PATH).rglob(\"data_test.hdf5\")))\nÂ  Â  with torch.no_grad():\nÂ  Â  Â  Â  for f_path in tqdm(test_files, desc=\"Inference\"):\nÂ  Â  Â  Â  Â  Â  with h5py.File(f_path, \"r\") as hf:\nÂ  Â  Â  Â  Â  Â  Â  Â  keys = sorted(hf.keys(), key=lambda k: int(k.split('_')[1]) if '_' in k else 0)\nÂ  Â  Â  Â  Â  Â  Â  Â  for key in keys:\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  x = torch.from_numpy(hf[key][\"input_features\"][()]).float().unsqueeze(0).to(DEVICE)\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  x = (x - x.mean()) / (x.std() + 1e-6)\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  logits = model(x)\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  indices = torch.argmax(logits[0], dim=-1).unique_consecutive()\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  decoded = \"\".join([CHARS[i] for i in indices if i != 0]).strip()\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  results.append(decoded if decoded else \"silence\")\nÂ  Â Â \nÂ  Â  pd.DataFrame({\"id\": range(len(results)), \"text\": results}).to_csv(\"submission.csv\", index=False)\nÂ  Â  print(\"ğŸ ØªÛ•ÙˆØ§Ùˆ! ÙØ§ÛŒÙ„ÛŒ submission.csv Ø¯Ø±ÙˆØ³Øª Ø¨ÙˆÙˆ.\")\n\nif __name__ == \"__main__\":\nÂ  Â  start_process()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Brain-to-Text 2025: Transformer-based Speech Reconstruction\n### Approach: 1D-CNN + Transformer Encoder + 1D-Upsampling\n\nThis notebook presents an innovative approach to decoding ECoG brain signals into text. Instead of traditional RNNs, I utilized a **Transformer architecture** combined with **1D-Upsampling** to handle the high-dimensional neural data.\n\n**Key Features:**\n* **Signal Processing:** Normalization and windowing of ECoG channels.\n* **Architecture:** 1D Convolutional layers for feature extraction followed by a Transformer Encoder for temporal dependency modeling.\n* **Decoding:** CTC Loss-based decoding to map neural features directly to character sequences.\n* **Innovation:** Implemented a custom 1D-Upsampling block to maintain spatial-temporal alignment between brain activity and text output.\n\n**Author:** Skala Hamasaed Fatah","metadata":{}}]}