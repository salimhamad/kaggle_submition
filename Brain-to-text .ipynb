{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":106809,"databundleVersionId":13056355,"sourceType":"competition"}],"dockerImageVersionId":31236,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import h5py, torch, os\nimport numpy as np\nimport pandas as pd\nfrom pathlib import Path\nfrom tqdm.auto import tqdm\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\n\n# --- Ù¡. Ú•ÛŽÚ©Ø®Ø³ØªÙ†ÛŒ Ø¨Ù†Û•Ú•Û•ØªÛŒ ---\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nPHONEMES = ['BLANK', 'AA', 'AE', 'AH', 'AO', 'AW', 'AY', 'B', 'CH', 'D', 'DH', 'EH', 'ER', 'EY', 'F', 'G', 'HH', 'IH', 'IY', 'JH', 'K', 'L', 'M', 'N', 'NG', 'OW', 'OY', 'P', 'R', 'S', 'SH', 'T', 'TH', 'UH', 'UW', 'V', 'W', 'Y', 'Z', 'ZH', 'SIL']\nPHONEME_MAP = {p: i for i, p in enumerate(PHONEMES)}\n\nTRAIN_FILES = [\n    '/kaggle/input/brain-to-text-25/t15_copyTask_neuralData/hdf5_data_final/t15.2025.03.14/data_train.hdf5',\n    '/kaggle/input/brain-to-text-25/t15_copyTask_neuralData/hdf5_data_final/t15.2023.08.11/data_train.hdf5',\n    '/kaggle/input/brain-to-text-25/t15_copyTask_neuralData/hdf5_data_final/t15.2023.11.19/data_train.hdf5'\n]\n\n# --- Ù¢. Ø¯Ø±ÙˆØ³ØªÚ©Ø±Ø¯Ù†ÛŒ Dataset (Ø¨Û•Ø´ÛŒ Ú†Ø§Ú©Ú©Ø±Ø§Ùˆ) ---\nclass BrainDataset(Dataset):\n    def __init__(self, file_paths):\n        self.data = []\n        for path in file_paths:\n            if not os.path.exists(path): continue\n            with h5py.File(path, 'r') as hf:\n                for k in hf.keys():\n                    feat = hf[k]['input_features'][()]\n                    \n                    # Ù„ÛŽØ±Û•Ø¯Ø§ Ú©ÛŽØ´Û•Ú©Û• Ù‡Û•Ø¨ÙˆÙˆØŒ Ø¦ÛŽØ³ØªØ§ Ú†Ø§Ú©Ú©Ø±Ø§ÙˆÛ•:\n                    raw_text = hf[k]['transcription'][()]\n                    \n                    # Ù¾Ø´Ú©Ù†ÛŒÙ†: Ø¦Û•Ú¯Û•Ø± Ø¨Ø§ÛŒØª Ø¨ÙˆÙˆ Ø¯ÛŒÚ©Û†Ø¯ÛŒ Ø¨Ú©Û•ØŒ Ø¦Û•Ú¯Û•Ø± Ù†Ø§ Ú•Ø§Ø³ØªÛ•ÙˆØ®Û† Ø¨Û•Ú©Ø§Ø±ÛŒØ¨Ù‡ÛŽÙ†Û•\n                    if isinstance(raw_text, bytes):\n                        p_text = raw_text.decode('utf-8').split()\n                    elif isinstance(raw_text, np.ndarray):\n                        # Ø¦Û•Ú¯Û•Ø± Ø¦Û•Ú•Û•ÛŒ Ø¨ÙˆÙˆØŒ Ø¯Û•ÛŒÚ©Û•ÛŒÙ† Ø¨Û• ØªÛŽÚ©Ø³Øª\n                        p_text = \" \".join([x.decode('utf-8') if isinstance(x, bytes) else str(x) for x in raw_text]).split()\n                    else:\n                        p_text = str(raw_text).split()\n                        \n                    target = [PHONEME_MAP.get(p, 41) for p in p_text]\n                    self.data.append((feat, target))\n\n    def __len__(self): return len(self.data)\n    def __getitem__(self, idx):\n        x, y = self.data[idx]\n        return torch.tensor(x).float(), torch.tensor(y).long()\n\n# --- Ù£. Ù…Û†Ø¯ÛŽÙ„ ---\nclass BrainCTCModel(nn.Module):\n    def __init__(self, input_dim=512, hidden_dim=256):\n        super().__init__()\n        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers=3, bidirectional=True, batch_first=True)\n        self.fc = nn.Linear(hidden_dim * 2, len(PHONEMES))\n    def forward(self, x):\n        x, _ = self.lstm(x)\n        return self.fc(x).log_softmax(2)\n\n# --- Ù¤. Ù…Û•Ø´Ù‚Ù¾ÛŽÚ©Ø±Ø¯Ù† ---\ndef train():\n    dataset = BrainDataset(TRAIN_FILES)\n    model = BrainCTCModel().to(DEVICE)\n    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n    criterion = nn.CTCLoss(blank=0, zero_infinity=True)\n\n    print(f\"ðŸš€ Ø¯Û•Ø³ØªÚ©Ø±Ø¯Ù† Ø¨Û• Ù…Û•Ø´Ù‚ Ù„Û•Ø³Û•Ø± {len(dataset)} Ù†Ù…ÙˆÙˆÙ†Û•...\")\n    model.train()\n    for epoch in range(10): \n        epoch_loss = 0\n        for x, y in dataset:\n            x, y = x.unsqueeze(0).to(DEVICE), y.unsqueeze(0).to(DEVICE)\n            optimizer.zero_grad()\n            log_probs = model(x).permute(1, 0, 2)\n            input_len = torch.tensor([log_probs.size(0)], dtype=torch.long)\n            target_len = torch.tensor([y.size(1)], dtype=torch.long)\n            loss = criterion(log_probs, y, input_len, target_len)\n            loss.backward()\n            optimizer.step()\n            epoch_loss += loss.item()\n        print(f\"Epoch {epoch+1} | Loss: {epoch_loss/len(dataset):.4f}\")\n    return model\n\n# --- Ù¥. Ù¾ÛŽØ´Ø¨ÛŒÙ†ÛŒ ---\ndef submit(model):\n    model.eval()\n    results = []\n    TEST_DIR = Path('/kaggle/input/brain-to-text-25/t15_copyTask_neuralData/hdf5_data_final')\n    test_files = sorted(list(TEST_DIR.rglob(\"data_test.hdf5\")))\n    with torch.no_grad():\n        for p in tqdm(test_files):\n            with h5py.File(p, \"r\") as hf:\n                trials = sorted(hf.keys(), key=lambda k: int(k.split('_')[1]) if '_' in k else 0)\n                for t in trials:\n                    x = torch.from_numpy(hf[t][\"input_features\"][()]).float().unsqueeze(0).to(DEVICE)\n                    logits = model(x)\n                    indices = torch.argmax(logits[0], dim=-1).unique_consecutive()\n                    pred = [PHONEMES[i] for i in indices if i != 0 and i < len(PHONEMES)]\n                    results.append(\" \".join(pred) if pred else \"SIL\")\n    pd.DataFrame({\"id\": range(len(results)), \"text\": results}).to_csv(\"submission.csv\", index=False)\n    print(\"ðŸ ØªÛ•ÙˆØ§Ùˆ! ÙØ§ÛŒÙ„Û•Ú©Û• Ø¦Ø§Ù…Ø§Ø¯Û•ÛŒÛ•.\")\n\ntrained_model = train()\nsubmit(trained_model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-30T20:08:36.996763Z","iopub.execute_input":"2025-12-30T20:08:36.997049Z","execution_failed":"2025-12-30T20:09:19.255Z"}},"outputs":[],"execution_count":null}]}