{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":106809,"databundleVersionId":13056355,"sourceType":"competition"}],"dockerImageVersionId":31236,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import h5py\nimport torch\nimport os\nimport numpy as np\nimport pandas as pd\nfrom pathlib import Path\nfrom tqdm.auto import tqdm\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\n\n# --- 1. Global Configurations and Device Allocation ---\n# Utilizing CUDA if available for accelerated deep learning performance\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nBASE_PATH = '/kaggle/input/brain-to-text-25/t15_copyTask_neuralData/hdf5_data_final'\n\n# Defining vocabulary: Using character-level tokens for higher granularity\nCHARS = ['BLANK'] + list(\" abcdefghijklmnopqrstuvwxyz'\") \nCHAR_MAP = {c: i for i, c in enumerate(CHARS)}\n\n# Dynamic discovery of all available training HDF5 files across the dataset directory\nTRAIN_FILES = [str(p) for p in Path(BASE_PATH).rglob('data_train.hdf5')]\n\n# --- 2. Advanced Transformer-based Architecture ---\n# Implementing a Multi-Head Self-Attention Encoder to capture long-range temporal dependencies\nclass NeuralTransformerModel(nn.Module):\n    def __init__(self, input_dim=512, hidden_dim=256, nhead=8, num_layers=6):\n        super(NeuralTransformerModel, self).__init__()\n        \n        # Initial projection layer to align neural features with transformer dimensionality\n        self.feature_projection = nn.Linear(input_dim, hidden_dim)\n        \n        # Transformer Encoder Layer with 8 attention heads for diverse feature extraction\n        encoder_layer = nn.TransformerEncoderLayer(\n            d_model=hidden_dim, \n            nhead=nhead, \n            dim_feedforward=hidden_dim * 4, \n            dropout=0.2, \n            batch_first=True\n        )\n        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n        \n        # Output classifier projecting hidden states to character probability space\n        self.final_classifier = nn.Linear(hidden_dim, len(CHARS))\n\n    def forward(self, x):\n        # x: [Batch, Sequence_Length, Input_Dim]\n        x = self.feature_projection(x)\n        x = self.transformer_encoder(x)\n        # Apply Log-Softmax for CTC Loss compatibility\n        return self.final_classifier(x).log_softmax(2)\n\n# --- 3. Data Engineering and Signal Preprocessing ---\nclass NeuralSignalDataset(Dataset):\n    def __init__(self, file_paths):\n        self.samples = []\n        for path in tqdm(file_paths, desc=\"Preprocessing Neural Data\"):\n            if not os.path.exists(path): continue\n            with h5py.File(path, 'r') as hf:\n                for key in hf.keys():\n                    # Load high-dimensional neural input features\n                    features = hf[key]['input_features'][()]\n                    \n                    # Apply Z-score Normalization to stabilize signal variance\n                    features = (features - np.mean(features)) / (np.std(features) + 1e-6)\n                    \n                    # Extract and decode transcription labels\n                    raw_text = hf[key].get('sentence', [b''])[0].decode('utf-8').lower()\n                    if not raw_text: continue\n                    \n                    # Convert characters to indices based on CHAR_MAP\n                    target_indices = [CHAR_MAP[c] for c in raw_text if c in CHAR_MAP]\n                    self.samples.append((features, target_indices))\n\n    def __len__(self):\n        return len(self.samples)\n\n    def __getitem__(self, idx):\n        features, target = self.samples[idx]\n        return torch.tensor(features).float(), torch.tensor(target).long()\n\n# --- 4. Optimization and Training Routine ---\ndef execute_training_pipeline():\n    print(f\"Status: Initializing Dataset with {len(TRAIN_FILES)} files...\")\n    dataset = NeuralSignalDataset(TRAIN_FILES)\n    \n    model = NeuralTransformerModel().to(DEVICE)\n    \n    # Utilizing AdamW optimizer for weight decay regularization\n    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=0.01)\n    \n    # Connectionist Temporal Classification (CTC) loss for unaligned sequences\n    criterion = nn.CTCLoss(blank=0, zero_infinity=True)\n\n    print(\"Status: Starting Model Optimization (50 Epochs Target)...\")\n    model.train()\n    \n    for epoch in range(50):\n        epoch_loss = 0\n        for features, targets in dataset:\n            features = features.unsqueeze(0).to(DEVICE)\n            targets = targets.unsqueeze(0).to(DEVICE)\n            \n            optimizer.zero_grad()\n            \n            # Forward pass: log-probabilities shape [Seq_Len, Batch, Classes] for CTC\n            output = model(features).permute(1, 0, 2)\n            \n            input_lengths = torch.tensor([output.size(0)], dtype=torch.long)\n            target_lengths = torch.tensor([targets.size(1)], dtype=torch.long)\n            \n            loss = criterion(output, targets, input_lengths, target_lengths)\n            loss.backward()\n            optimizer.step()\n            \n            epoch_loss += loss.item()\n            \n        avg_loss = epoch_loss / len(dataset)\n        print(f\"Epoch [{epoch+1}/50] - Training Loss: {avg_loss:.4f}\")\n        \n    return model\n\n# --- 5. Inference and Submission Generation ---\ndef generate_submission(model):\n    model.eval()\n    predictions = []\n    test_files = sorted(list(Path(BASE_PATH).rglob(\"data_test.hdf5\")))\n    \n    print(\"Status: Commencing Inference on Test Partitions...\")\n    with torch.no_grad():\n        for file_path in tqdm(test_files):\n            with h5py.File(file_path, \"r\") as hf:\n                trial_keys = sorted(hf.keys(), key=lambda k: int(k.split('_')[1]) if '_' in k else 0)\n                for key in trial_keys:\n                    x = torch.from_numpy(hf[key][\"input_features\"][()]).float().unsqueeze(0).to(DEVICE)\n                    # Apply identical normalization as training\n                    x = (x - x.mean()) / (x.std() + 1e-6)\n                    \n                    logits = model(x)\n                    \n                    # Greedy decoding: Select highest probability indices and collapse repeats\n                    best_path = torch.argmax(logits[0], dim=-1).unique_consecutive()\n                    decoded_str = \"\".join([CHARS[i] for i in best_path if i != 0])\n                    predictions.append(decoded_str.strip() if decoded_str.strip() else \"sil\")\n\n    # Construct final dataframe for Kaggle submission\n    submission_df = pd.DataFrame({\"id\": range(len(predictions)), \"text\": predictions})\n    submission_df.to_csv(\"submission.csv\", index=False)\n    print(\"Final Status: Submission file 'submission.csv' generated successfully.\")\n\n# --- System Execution ---\nif __name__ == \"__main__\":\n    optimized_model = execute_training_pipeline()\n    generate_submission(optimized_model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-31T00:45:54.982387Z","iopub.execute_input":"2025-12-31T00:45:54.982685Z"}},"outputs":[{"name":"stdout","text":"Status: Initializing Dataset with 45 files...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Preprocessing Neural Data:   0%|          | 0/45 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2cfe1934023948a19f53fc4cc2e18006"}},"metadata":{}}],"execution_count":null},{"cell_type":"code","source":"import h5py, torch, os\nimport numpy as np\nimport pandas as pd\nfrom pathlib import Path\nfrom tqdm.auto import tqdm\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\n\n# --- Ù¡. Ú•ÛŽÚ©Ø®Ø³ØªÙ†ÛŒ Ø¨Ù†Û•Ú•Û•ØªÛŒ ---\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nBASE_PATH = '/kaggle/input/brain-to-text-25/t15_copyTask_neuralData/hdf5_data_final'\nCHARS = ['BLANK'] + list(\" abcdefghijklmnopqrstuvwxyz'\") \nCHAR_MAP = {c: i for i, c in enumerate(CHARS)}\n\nTRAIN_FILES = [str(p) for p in Path(BASE_PATH).rglob('data_train.hdf5')]\n\n# --- Ù¢. Ù…Û†Ø¯ÛŽÙ„ÛŒ Ø²Û†Ø± Ù¾ÛŽØ´Ú©Û•ÙˆØªÙˆÙˆ (Conformer Architecture) ---\n# Ø¦Û•Ù… Ù…Û†Ø¯ÛŽÙ„Û• Ø³ÛŒÚ¯Ù†Ø§ÚµÛ•Ú©Ø§Ù† Ø¨Û• ÙˆØ±Ø¯ÛŒ \"Ø³Û•Ø±Ù†Ø¬\" (Attention) Ø¯Û•Ø¯Ø§Øª\n\nclass BrainConformerModel(nn.Module):\n    def __init__(self, input_dim=512, hidden_dim=256, nhead=8, num_layers=6):\n        super().__init__()\n        # Ú†ÛŒÙ†ÛŒ Ø³Û•Ø±Û•ØªØ§ Ø¨Û† Ú¯Û†Ú•ÛŒÙ†ÛŒ Ø³ÛŒÚ¯Ù†Ø§Úµ Ø¨Û† ÙÛ•Ø²Ø§ÛŒ Transformer\n        self.embedding = nn.Linear(input_dim, hidden_dim)\n        \n        # Ú†ÛŒÙ†ÛŒ Transformer Encoder Ø¨Û† Ø¯Û†Ø²ÛŒÙ†Û•ÙˆÛ•ÛŒ Ù¾Û•ÛŒÙˆÛ•Ù†Ø¯ÛŒÛŒÛ• Ø¦Ø§ÚµÛ†Ø²Û•Ú©Ø§Ù†\n        encoder_layer = nn.TransformerEncoderLayer(\n            d_model=hidden_dim, nhead=nhead, dim_feedforward=hidden_dim*4, \n            dropout=0.2, batch_first=True\n        )\n        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n        \n        # Ú†ÛŒÙ†ÛŒ Ú©Û†ØªØ§ÛŒÛŒ Ø¨Û† Ù¾ÛŽØ´Ø¨ÛŒÙ†ÛŒ Ù¾ÛŒØªÛ•Ú©Ø§Ù†\n        self.classifier = nn.Linear(hidden_dim, len(CHARS))\n\n    def forward(self, x):\n        x = self.embedding(x)\n        x = self.transformer(x)\n        return self.classifier(x).log_softmax(2)\n\n# --- Ù£. Ø¦Ø§Ù…Ø§Ø¯Û•Ú©Ø±Ø¯Ù†ÛŒ Ø¯Ø§ØªØ§ (Dataset) ---\nclass BrainDataset(Dataset):\n    def __init__(self, file_paths):\n        self.data = []\n        for path in tqdm(file_paths, desc=\"Loading Data\"):\n            with h5py.File(path, 'r') as hf:\n                for k in hf.keys():\n                    feat = hf[k]['input_features'][()]\n                    # Ø¦Ø§Ø³Ø§ÛŒÛŒÚ©Ø±Ø¯Ù†Û•ÙˆÛ•ÛŒ Ø³ÛŒÚ¯Ù†Ø§Úµ (Z-score normalization) Ø²Û†Ø± Ú¯Ø±Ù†Ú¯Û• Ø¨Û† Ø¨Ø±Ø¯Ù†Û•ÙˆÛ•\n                    feat = (feat - np.mean(feat)) / (np.std(feat) + 1e-6)\n                    \n                    sentence = hf[k].get('sentence', [b''])[0].decode('utf-8').lower()\n                    if not sentence: continue\n                    target = [CHAR_MAP[c] for c in sentence if c in CHAR_MAP]\n                    self.data.append((feat, target))\n\n    def __len__(self): return len(self.data)\n    def __getitem__(self, idx):\n        return torch.tensor(self.data[idx][0]).float(), torch.tensor(self.data[idx][1]).long()\n\n# --- Ù¤. Ù…Û•Ø´Ù‚ÛŒ Ú†Ú• Ø¨Û† Ø¨Ø±Ø¯Ù†Û•ÙˆÛ• (Training) ---\ndef train_to_win():\n    dataset = BrainDataset(TRAIN_FILES)\n    model = BrainConformerModel().to(DEVICE)\n    \n    # Ø¨Û•Ú©Ø§Ø±Ù‡ÛŽÙ†Ø§Ù†ÛŒ OneCycleLR Ø¨Û† Ø¦Û•ÙˆÛ•ÛŒ Ù…Û†Ø¯ÛŽÙ„Û•Ú©Û• Ø®ÛŽØ±Ø§ØªØ± Ùˆ Ø¨Ø§Ø´ØªØ± ÙÛŽØ± Ø¨ÛŽØª\n    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=0.01)\n    criterion = nn.CTCLoss(blank=0, zero_infinity=True)\n\n    print(\"ðŸ”¥ Ù…Û•Ø´Ù‚Ú©Ø±Ø¯Ù† Ø¨Û• Ù…Û†Ø¯ÛŽÙ„ÛŒ Conformer Ø¯Û•Ø³ØªÛŒ Ù¾ÛŽÚ©Ø±Ø¯...\")\n    model.train()\n    for epoch in range(50): # Ù¥Ù  Ø®ÙˆÙ„ Ø¨Û† Ø¦Û•ÙˆÛ•ÛŒ Ù…Û†Ø¯ÛŽÙ„Û•Ú©Û• Ù‡Û•Ù…ÙˆÙˆ Ø´ØªÛŽÚ© ÙÛŽØ± Ø¨ÛŽØª\n        total_loss = 0\n        for x, y in dataset:\n            x, y = x.unsqueeze(0).to(DEVICE), y.unsqueeze(0).to(DEVICE)\n            optimizer.zero_grad()\n            \n            log_probs = model(x).permute(1, 0, 2)\n            input_len = torch.tensor([log_probs.size(0)], dtype=torch.long)\n            target_len = torch.tensor([y.size(1)], dtype=torch.long)\n            \n            loss = criterion(log_probs, y, input_len, target_len)\n            loss.backward()\n            optimizer.step()\n            total_loss += loss.item()\n        \n        print(f\"Epoch {epoch+1}/50 | Loss: {total_loss/len(dataset):.4f}\")\n    return model\n\n# --- Ù¥. Ù†Ø§Ø±Ø¯Ù†ÛŒ Ø¦Û•Ù†Ø¬Ø§Ù… (Submission) ---\ndef final_submit(model):\n    model.eval()\n    results = []\n    test_files = sorted(list(Path(BASE_PATH).rglob(\"data_test.hdf5\")))\n    \n    with torch.no_grad():\n        for p in tqdm(test_files):\n            with h5py.File(p, \"r\") as hf:\n                trials = sorted(hf.keys(), key=lambda k: int(k.split('_')[1]) if '_' in k else 0)\n                for t in trials:\n                    x = torch.from_numpy(hf[t][\"input_features\"][()]).float().unsqueeze(0).to(DEVICE)\n                    # Normalization Ø¨Û† ØªÛŽØ³ØªÛŒØ´ Ù¾ÛŽÙˆÛŒØ³ØªÛ•\n                    x = (x - x.mean()) / (x.std() + 1e-6)\n                    \n                    logits = model(x)\n                    indices = torch.argmax(logits[0], dim=-1).unique_consecutive()\n                    pred = \"\".join([CHARS[i] for i in indices if i != 0])\n                    results.append(pred.strip() if pred.strip() else \"sil\")\n\n    pd.DataFrame({\"id\": range(len(results)), \"text\": results}).to_csv(\"submission.csv\", index=False)\n    print(\"ðŸ† ØªÛ•ÙˆØ§Ùˆ! Ù‡ÛŒÙˆØ§Ø¯Ø§Ø±Ù… Ø¦Û•Ù…Û• Ù†Ù…Ø±Û•ÛŒ ÛŒÛ•Ú©Û•Ù…Øª Ø¨Û† Ø¨Ù‡ÛŽÙ†ÛŽØª.\")\n\n# Ú©Ø§Ø±Ù¾ÛŽÚ©Ø±Ø¯Ù†\nwinning_model = train_to_win()\nfinal_submit(winning_model)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}