{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":106809,"databundleVersionId":13056355,"sourceType":"competition"}],"dockerImageVersionId":31236,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import h5py, torch, os\nimport numpy as np\nimport pandas as pd\nfrom pathlib import Path\nfrom tqdm.auto import tqdm\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\n\n# --- Ù¡. Ú•ÛÚ©Ø®Ø³ØªÙ†ÛŒ Ø¨Ù†Û•Ú•Û•ØªÛŒ Ùˆ Ø¯Û†Ø²ÛŒÙ†Û•ÙˆÛ•ÛŒ Ù‡Û•Ù…ÙˆÙˆ Ø¯Ø§ØªØ§Ú©Ø§Ù† ---\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nBASE_PATH = '/kaggle/input/brain-to-text-25/t15_copyTask_neuralData/hdf5_data_final'\n\nPHONEMES = ['BLANK', 'AA', 'AE', 'AH', 'AO', 'AW', 'AY', 'B', 'CH', 'D', 'DH', 'EH', 'ER', 'EY', 'F', 'G', 'HH', 'IH', 'IY', 'JH', 'K', 'L', 'M', 'N', 'NG', 'OW', 'OY', 'P', 'R', 'S', 'SH', 'T', 'TH', 'UH', 'UW', 'V', 'W', 'Y', 'Z', 'ZH', 'SIL']\nPHONEME_MAP = {p: i for i, p in enumerate(PHONEMES)}\n\n# Ø¯Û†Ø²ÛŒÙ†Û•ÙˆÛ•ÛŒ Ù‡Û•Ù…ÙˆÙˆ ÙØ§ÛŒÙ„Û•Ú©Ø§Ù†ÛŒ Ù…Û•Ø´Ù‚ Ø¨Û• Ø´ÛÙˆÛ•ÛŒ Ø¯Ø§ÛŒÙ†Ø§Ù…ÛŒÚ©ÛŒ\nTRAIN_FILES = [str(p) for p in Path(BASE_PATH).rglob('data_train.hdf5')]\nprint(f\"âœ… Ø¯Û†Ø²ÛŒÙ†Û•ÙˆÛ•ÛŒ {len(TRAIN_FILES)} ÙØ§ÛŒÙ„ÛŒ Ù…Û•Ø´Ù‚ Ø¨Û† Ú•Ø§Ù‡ÛÙ†Ø§Ù†.\")\n\n# --- Ù¢. Ø¯Ø±ÙˆØ³ØªÚ©Ø±Ø¯Ù†ÛŒ Dataset Ø¨Û† Ø®ÙˆÛÙ†Ø¯Ù†Û•ÙˆÛ•ÛŒ Ù‡Û•Ù…ÙˆÙˆ Ù†Ù…ÙˆÙˆÙ†Û•Ú©Ø§Ù† ---\nclass BrainDataset(Dataset):\n    def __init__(self, file_paths):\n        self.data = []\n        for path in tqdm(file_paths, desc=\"Loading Data\"):\n            if not os.path.exists(path): continue\n            with h5py.File(path, 'r') as hf:\n                for k in hf.keys():\n                    feat = hf[k]['input_features'][()]\n                    raw_text = hf[k]['transcription'][()]\n                    \n                    if isinstance(raw_text, bytes):\n                        p_text = raw_text.decode('utf-8').split()\n                    elif isinstance(raw_text, np.ndarray):\n                        p_text = \" \".join([x.decode('utf-8') if isinstance(x, bytes) else str(x) for x in raw_text]).split()\n                    else:\n                        p_text = str(raw_text).split()\n                        \n                    target = [PHONEME_MAP.get(p, 41) for p in p_text]\n                    self.data.append((feat, target))\n\n    def __len__(self): return len(self.data)\n    def __getitem__(self, idx):\n        x, y = self.data[idx]\n        return torch.tensor(x).float(), torch.tensor(y).long()\n\n# --- Ù£. Ù…Û†Ø¯ÛÙ„ÛŒ Ø¨Û•Ù‡ÛØ²Ú©Ø±Ø§Ùˆ (Deep Bi-LSTM) ---\n# \nclass BrainCTCModel(nn.Module):\n    def __init__(self, input_dim=512, hidden_dim=512):\n        super().__init__()\n        # Ø¨Û•Ú©Ø§Ø±Ù‡ÛÙ†Ø§Ù†ÛŒ Ù£ Ú†ÛŒÙ† Ø¨Û† Ù‚ÙˆÙˆÚµØ¨ÙˆÙˆÙ†ÛŒ ÙÛØ±Ø¨ÙˆÙˆÙ†\n        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers=3, \n                            bidirectional=True, batch_first=True, dropout=0.3)\n        self.fc = nn.Linear(hidden_dim * 2, len(PHONEMES))\n\n    def forward(self, x):\n        x, _ = self.lstm(x)\n        return self.fc(x).log_softmax(2)\n\n# --- Ù¤. Ù¾Ø±Û†Ø³Û•ÛŒ Ù…Û•Ø´Ù‚Ù¾ÛÚ©Ø±Ø¯Ù†ÛŒ Ú†Ú• (Training) ---\ndef train():\n    dataset = BrainDataset(TRAIN_FILES)\n    model = BrainCTCModel().to(DEVICE)\n    \n    # Ø¨Û•Ú©Ø§Ø±Ù‡ÛÙ†Ø§Ù†ÛŒ AdamW Ù„Û•Ú¯Û•Úµ Weight Decay Ø¨Û† Ø¦Û•Ù†Ø¬Ø§Ù…ÛŒ Ø¬ÛÚ¯ÛŒØ±ØªØ±\n    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=0.01)\n    criterion = nn.CTCLoss(blank=0, zero_infinity=True)\n\n    print(f\"ğŸš€ Ø¯Û•Ø³ØªÚ©Ø±Ø¯Ù† Ø¨Û• Ù…Û•Ø´Ù‚ Ù„Û•Ø³Û•Ø± {len(dataset)} Ù†Ù…ÙˆÙˆÙ†Û• Ø¨Û† Ù£Ù  Ø®ÙˆÙ„...\")\n    model.train()\n    \n    for epoch in range(30): \n        epoch_loss = 0\n        for x, y in dataset:\n            x, y = x.unsqueeze(0).to(DEVICE), y.unsqueeze(0).to(DEVICE)\n            optimizer.zero_grad()\n            \n            log_probs = model(x).permute(1, 0, 2) # [T, N, C]\n            input_len = torch.tensor([log_probs.size(0)], dtype=torch.long)\n            target_len = torch.tensor([y.size(1)], dtype=torch.long)\n            \n            loss = criterion(log_probs, y, input_len, target_len)\n            loss.backward()\n            optimizer.step()\n            epoch_loss += loss.item()\n            \n        print(f\"Epoch {epoch+1}/30 | Loss: {epoch_loss/len(dataset):.4f}\")\n    return model\n\n# --- Ù¥. Ù¾ÛØ´Ø¨ÛŒÙ†ÛŒ Ùˆ Ø¯Ø±ÙˆØ³ØªÚ©Ø±Ø¯Ù†ÛŒ ÙØ§ÛŒÙ„ÛŒ Submission ---\n# \ndef submit(model):\n    model.eval()\n    results = []\n    test_files = sorted(list(Path(BASE_PATH).rglob(\"data_test.hdf5\")))\n    \n    print(\"ğŸ“ Ø¯Ø±ÙˆØ³ØªÚ©Ø±Ø¯Ù†ÛŒ Ù¾ÛØ´Ø¨ÛŒÙ†ÛŒÛŒÛ•Ú©Ø§Ù† Ø¨Û† ÙØ§ÛŒÙ„ÛŒ ØªÛØ³Øª...\")\n    with torch.no_grad():\n        for p in tqdm(test_files):\n            with h5py.File(p, \"r\") as hf:\n                trials = sorted(hf.keys(), key=lambda k: int(k.split('_')[1]) if '_' in k else 0)\n                for t in trials:\n                    x = torch.from_numpy(hf[t][\"input_features\"][()]).float().unsqueeze(0).to(DEVICE)\n                    logits = model(x)\n                    \n                    # Greedy Decoding\n                    indices = torch.argmax(logits[0], dim=-1).unique_consecutive()\n                    pred = [PHONEMES[i] for i in indices if i != 0 and i < len(PHONEMES)]\n                    results.append(\" \".join(pred) if pred else \"SIL\")\n                    \n    pd.DataFrame({\"id\": range(len(results)), \"text\": results}).to_csv(\"submission.csv\", index=False)\n    print(\"ğŸ ØªÛ•ÙˆØ§Ùˆ! Ø¦ÛØ³ØªØ§ ÙØ§ÛŒÙ„ÛŒ submission.csv Ø¦Ø§Ù…Ø§Ø¯Û•ÛŒÛ• Ø¨Û† Ù†Ø§Ø±Ø¯Ù†.\")\n\n# --- Ø¬ÛØ¨Û•Ø¬ÛÚ©Ø±Ø¯Ù† ---\ntrained_model = train()\nsubmit(trained_model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-31T00:30:58.844524Z","iopub.execute_input":"2025-12-31T00:30:58.845171Z"}},"outputs":[{"name":"stdout","text":"âœ… Ø¯Û†Ø²ÛŒÙ†Û•ÙˆÛ•ÛŒ 45 ÙØ§ÛŒÙ„ÛŒ Ù…Û•Ø´Ù‚ Ø¨Û† Ú•Ø§Ù‡ÛÙ†Ø§Ù†.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading Data:   0%|          | 0/45 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"de53ac42118d454390f4b69dd8aa2698"}},"metadata":{}}],"execution_count":null}]}