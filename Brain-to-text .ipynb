{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":106809,"databundleVersionId":13056355,"sourceType":"competition"}],"dockerImageVersionId":31236,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import h5py\nimport torch\nimport os\nimport numpy as np\nimport pandas as pd\nfrom pathlib import Path\nfrom tqdm.auto import tqdm\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\n\n# --- 1. Global Configurations and Device Allocation ---\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nBASE_PATH = '/kaggle/input/brain-to-text-25/t15_copyTask_neuralData/hdf5_data_final'\n\n# Academic Vocabulary: Character-level tokens including special characters\nCHARS = ['BLANK'] + list(\" abcdefghijklmnopqrstuvwxyz' \") \nCHAR_MAP = {c: i for i, c in enumerate(CHARS)}\n\n# Dynamic discovery of all available training HDF5 files\nTRAIN_FILES = [str(p) for p in Path(BASE_PATH).rglob('data_train.hdf5')]\n\n# --- 2. Robust Data Engineering and Signal Preprocessing ---\nclass NeuralSignalDataset(Dataset):\n    \"\"\"\n    Custom Dataset class designed to handle heterogeneous neural data structures.\n    Implements multi-key fallback to prevent ZeroDivisionError.\n    \"\"\"\n    def __init__(self, file_paths):\n        self.samples = []\n        for path in tqdm(file_paths, desc=\"Preprocessing Neural Data\"):\n            if not os.path.exists(path): continue\n            with h5py.File(path, 'r') as hf:\n                for key in hf.keys():\n                    try:\n                        # Load high-dimensional neural input features\n                        features = hf[key]['input_features'][()]\n                        \n                        # Apply Z-score Normalization for signal stabilization\n                        features = (features - np.mean(features)) / (np.std(features) + 1e-6)\n\n                        # Robust search for transcription/label keys\n                        text_data = None\n                        for label_key in ['transcription', 'sentence', 'phonemes']:\n                            if label_key in hf[key]:\n                                raw_val = hf[key][label_key][()]\n                                if isinstance(raw_val, bytes):\n                                    text_data = raw_val.decode('utf-8').lower()\n                                elif isinstance(raw_val, np.ndarray):\n                                    text_data = \" \".join([x.decode('utf-8') if isinstance(x, bytes) else str(x) for x in raw_val]).lower()\n                                else:\n                                    text_data = str(raw_val).lower()\n                                break\n                        \n                        if text_data:\n                            target_indices = [CHAR_MAP[c] for c in text_data if c in CHAR_MAP]\n                            if len(target_indices) > 0:\n                                self.samples.append((features, target_indices))\n                    except Exception:\n                        continue\n\n        if len(self.samples) == 0:\n            print(\"Warning: No valid samples found. Verify HDF5 internal structure.\")\n\n    def __len__(self):\n        return len(self.samples)\n\n    def __getitem__(self, idx):\n        features, target = self.samples[idx]\n        return torch.tensor(features).float(), torch.tensor(target).long()\n\n# --- 3. Advanced Transformer-based Architecture ---\n\nclass NeuralTransformerModel(nn.Module):\n    def __init__(self, input_dim=512, hidden_dim=256, nhead=8, num_layers=6):\n        super(NeuralTransformerModel, self).__init__()\n        self.feature_projection = nn.Linear(input_dim, hidden_dim)\n        \n        encoder_layer = nn.TransformerEncoderLayer(\n            d_model=hidden_dim, \n            nhead=nhead, \n            dim_feedforward=hidden_dim * 4, \n            dropout=0.2, \n            batch_first=True\n        )\n        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n        self.final_classifier = nn.Linear(hidden_dim, len(CHARS))\n\n    def forward(self, x):\n        x = self.feature_projection(x)\n        x = self.transformer_encoder(x)\n        return self.final_classifier(x).log_softmax(2)\n\n# --- 4. Optimization and Training Routine ---\ndef execute_training_pipeline():\n    print(f\"Status: Initializing Dataset with {len(TRAIN_FILES)} files...\")\n    dataset = NeuralSignalDataset(TRAIN_FILES)\n    \n    if len(dataset) == 0:\n        return None\n\n    model = NeuralTransformerModel().to(DEVICE)\n    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=0.01)\n    criterion = nn.CTCLoss(blank=0, zero_infinity=True)\n\n    print(f\"Status: Optimization commenced on {len(dataset)} samples (50 Epochs Target)...\")\n    model.train()\n    \n    for epoch in range(50):\n        epoch_loss = 0\n        for features, targets in dataset:\n            features = features.unsqueeze(0).to(DEVICE)\n            targets = targets.unsqueeze(0).to(DEVICE)\n            \n            optimizer.zero_grad()\n            output = model(features).permute(1, 0, 2) # [Seq, Batch, Class]\n            \n            input_lengths = torch.tensor([output.size(0)], dtype=torch.long)\n            target_lengths = torch.tensor([targets.size(1)], dtype=torch.long)\n            \n            loss = criterion(output, targets, input_lengths, target_lengths)\n            loss.backward()\n            optimizer.step()\n            epoch_loss += loss.item()\n            \n        print(f\"Epoch [{epoch+1}/50] - Average Training Loss: {epoch_loss / len(dataset):.4f}\")\n        \n    return model\n\n# --- 5. Inference and Submission Generation ---\n\ndef generate_submission(model):\n    if model is None: \n        print(\"Error: No trained model available for inference.\")\n        return\n\n    model.eval()\n    predictions = []\n    test_files = sorted(list(Path(BASE_PATH).rglob(\"data_test.hdf5\")))\n    \n    print(\"Status: Commencing Inference on Test Partitions...\")\n    with torch.no_grad():\n        for file_path in tqdm(test_files):\n            with h5py.File(file_path, \"r\") as hf:\n                trial_keys = sorted(hf.keys(), key=lambda k: int(k.split('_')[1]) if '_' in k else 0)\n                for key in trial_keys:\n                    x = torch.from_numpy(hf[key][\"input_features\"][()]).float().unsqueeze(0).to(DEVICE)\n                    x = (x - x.mean()) / (x.std() + 1e-6) # Consistency in normalization\n                    \n                    logits = model(x)\n                    best_path = torch.argmax(logits[0], dim=-1).unique_consecutive()\n                    decoded_str = \"\".join([CHARS[i] for i in best_path if i != 0])\n                    predictions.append(decoded_str.strip() if decoded_str.strip() else \"silence\")\n\n    submission_df = pd.DataFrame({\"id\": range(len(predictions)), \"text\": predictions})\n    submission_df.to_csv(\"submission.csv\", index=False)\n    print(\"Final Status: Submission file 'submission.csv' generated successfully.\")\n\n# --- System Execution ---\nif __name__ == \"__main__\":\n    optimized_model = execute_training_pipeline()\n    generate_submission(optimized_model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-31T00:50:17.105250Z","iopub.execute_input":"2025-12-31T00:50:17.105570Z"}},"outputs":[{"name":"stdout","text":"Status: Initializing Dataset with 45 files...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Preprocessing Neural Data:   0%|          | 0/45 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"41248de6e6ce468b86bc78cef2f21198"}},"metadata":{}},{"name":"stdout","text":"Status: Optimization commenced on 8072 samples (50 Epochs Target)...\n","output_type":"stream"}],"execution_count":null}]}